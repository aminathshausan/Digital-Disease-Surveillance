{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0654a5fc-ff77-4f2f-853f-677f28ebc8fc",
   "metadata": {},
   "source": [
    "Date:Feb 21 2023\n",
    "\n",
    "Author: Aminath Shausan\n",
    "\n",
    "This program cleans the Twitter (covid and influenza), CRISPER (covid) NNDSS (influenza) data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cb4e56-0211-4efa-8bfe-22d78af1d043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path = \"/Users/uqashaus/OneDrive - The University of Queensland/Git/DigitalDiseaseSurveillance\"\n",
    "path = '/Users/sha481/Documents/UQGitHub/DigitalDiseaseSurveillance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0af478a-4bbf-497c-be83-fad72ae14493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "#from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a42e4a-0903-4ca7-9b48-6371de556496",
   "metadata": {},
   "source": [
    "# Twitter data\n",
    "## Data Extraction\n",
    "\n",
    "Extract required columns from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa00df-178f-4ff1-baef-af28029b0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read original data and scrap required columns\n",
    "#check the col names of the returned csv\n",
    "def get_cols(yrname, filename):\n",
    "    df = pd.read_csv(path + '/data/Twitter/' + yrname + '/' + filename + '.csv', \n",
    "                    low_memory=False)\n",
    "    df_sub = df[['created_at', 'geo.full_name', 'geo.geo.bbox', 'geo.name', 'geo.place_type',]]\n",
    "\n",
    "    df_sub.columns  \n",
    "    df_sub.head()\n",
    "\n",
    "    #save data into csv\n",
    "    df_sub.to_csv(path + '/data/Twitter/sample/influenza/' + filename + '.csv', index = False) \n",
    "    \n",
    "    return  df_sub #df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d91f4-e13b-42bd-a470-8753b8931540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_cols('influenza2020', 'Jan20') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffccf47-36d7-4b67-a035-c4ddac60f8bd",
   "metadata": {},
   "source": [
    "## Perform basic cleaning \n",
    "Format date and full name columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b9c90-b5b3-4706-833b-ece02e42cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(filename):\n",
    "    df = pd.read_csv(path + '/data/Twitter/sample/influenza/' + filename + '.csv',   \n",
    "                    low_memory=False) \n",
    "    #remove double quoetes\n",
    "    df = df.replace('\"', '', regex=True) \n",
    "    #remove everything after T in datetime column\n",
    "    df['created_at'] = df['created_at'].str.split('T').str[0]\n",
    "    #change date column to datetime format\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d')\n",
    "    #find states \n",
    "    df['state'] = df['geo.full_name'].str.split(' ').str[1]\n",
    "    #rename columns \n",
    "    cols = ['Date','GeoFullName','GeoBox', 'GeoName', 'GeoPlaceType', 'State']\n",
    "    df.columns = cols\n",
    "    #save df\n",
    "    df.to_csv(path + '/data/Twitter/sample/clean_influenza/' + filename + '_basic.csv', index = False)\n",
    "    return df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fa36f-8a15-4ee0-91ab-fed9cb0ac625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = basic_clean('Jan20')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a47e3-c2b2-4346-bc80-6083497cc531",
   "metadata": {},
   "source": [
    "## Check Basic Cleaned dataset \n",
    "\n",
    "Note: the State column in the basic clean dataset sometimes contains city names. \n",
    "So replace them with their states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1738e4-ad46-46bf-9e9b-24fe183f69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the basic clean data and check unique names in the State column \n",
    "def get_unique(filename):\n",
    "    df = pd.read_csv(path + '/data/Twitter/sample/clean_influenza/' +  filename + '.csv', \n",
    "                    low_memory=False) #,  engine='python' \n",
    "    return  df['GeoName'].unique()  #use this with influenza data \n",
    "#df['State'].unique() use this with with covid data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ff3de-a700-40f0-821b-8cca63ee34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = get_unique('June18_basic')\n",
    "print(unique_states)\n",
    "type(unique_states) #numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d313a2a-45a8-4660-89d0-4cc0e62e02f3",
   "metadata": {},
   "source": [
    "## Create states from GeoFullName column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32a8ca-5c1e-4691-a775-43311276084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if a list of states, cities are in the GeoFullName column\n",
    "\n",
    "\n",
    "#df_sub = df.head(10)\n",
    "#df_sub\n",
    "\n",
    "lst_act = ['Australian Capital Territory' , 'Canberra', 'ACT']\n",
    "lst_nt= ['Northern Territory' , 'Darwin', 'NT']\n",
    "lst_nsw = ['New South Wales', 'Sydney']\n",
    "lst_qld = ['Queensland', 'Brisbane', 'Gold Coast', 'QLD']\n",
    "lst_vic = ['Victoria', 'Melbourn', 'VIC']\n",
    "lst_sa = ['South Australia', 'Adelaide', 'SA']\n",
    "lst_wa = ['Western Australia', 'Perth', 'WA']\n",
    "lst_ta = ['Tasmania', 'Hobart', 'TAS']\n",
    "\n",
    "def create_states(filename):\n",
    "    \n",
    "    global lst_act, lst_nt, lst_nsw, lst_qld, lst_vic, lst_sa, lst_wa, lst_ta\n",
    "    df = pd.read_csv(path + '/data/Twitter/sample/clean_influenza/' +filename +\n",
    "                     '.csv', low_memory=False)\n",
    "    df['GeoFullName']= df['GeoFullName'].fillna('')\n",
    "    #df['GeoFullName']=  df['GeoFullName'].str.replace(' ', '')\n",
    "    geoFullName = df['GeoFullName']\n",
    "    #print(len(geoFullName))\n",
    "    df['State']= df['GeoName'].fillna('')\n",
    "    print(df.shape)\n",
    "    \n",
    "    for i in range(0, len(geoFullName)): \n",
    "        if any(actName in geoFullName[i] for actName in lst_act):\n",
    "            df['State'][i]=  'Australian Capital Territory'\n",
    "        elif any(ntName in geoFullName[i] for ntName in lst_nt):\n",
    "            df['State'][i]=  'Northern Territory'\n",
    "        elif any(nswName in geoFullName[i] for nswName in lst_nsw):\n",
    "            df['State'][i]=  'New South Wales'\n",
    "        elif any(qldName in geoFullName[i] for qldName in lst_qld):\n",
    "            df['State'][i]=  'Queensland'\n",
    "        elif any(vicName in geoFullName[i] for vicName in lst_vic):\n",
    "            df['State'][i]=  'Victoria'\n",
    "        elif any(saName in geoFullName[i] for saName in lst_sa):\n",
    "            df['State'][i]=  'South Australia'\n",
    "        elif any(waName in geoFullName[i] for waName in lst_wa):\n",
    "            df['State'][i]=  'Western Australia'\n",
    "        elif any(taName in geoFullName[i] for taName in lst_ta):\n",
    "            df['State'][i]=  'Tasmania'\n",
    "            \n",
    "    df.to_csv(path + '/data/Twitter/sample/clean_influenza/' + filename + '.csv', index = False)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec39e21-eb8f-41ff-bb1e-e0eb06dc904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_states('Apr20_basic')\n",
    "df['State'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458e8c9-ef02-4af9-b543-c5c67cac408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of locations and states : \n",
    "\n",
    "location = {'Adelaide': 'South Australia',\n",
    "            'Agnes Water': 'Queensland',\n",
    "            'Armadale': 'Victoria',\n",
    "            'Altona North': 'Victoria',\n",
    "            'Alderley': 'Queensland',\n",
    "            'Angle Vale': 'South Australia',\n",
    "            'Ashby': 'New South Wales',\n",
    "            'Albany': 'Western Australia',\n",
    "            'Anna Bay': 'New South Wales',\n",
    "            'Arrawarra': 'New South Wales',\n",
    "            'Alice Springs': 'Northern Territory',\n",
    "            'Albury':'New South Wales',\n",
    "            'Armidale': 'New South Wales',\n",
    "            'Acton': 'Australian Capital Territory',\n",
    "            'Bathurst': 'New South Wales',\n",
    "            'Bardon': 'Queensland',\n",
    "            'Beaconsfield Upper': 'Victoria',\n",
    "            'Bunbury': 'Western Australia',\n",
    "            'Brunswick Heads': 'New South Wales',\n",
    "            'Bundaberg': 'Queensland', \n",
    "            'Benalla': 'Victoria',\n",
    "            'Berri': 'South Australia',\n",
    "            'Bellingen': 'New South Wales',\n",
    "            'Brisbane': 'Queensland',\n",
    "            'Blue Mountains': 'New South Wales',\n",
    "            'Blue Mountain Heights': 'New South Wales',\n",
    "            'Ballarat': 'Victoria',\n",
    "            'Byron Bay': 'Queensland',\n",
    "            'Bungendore': 'New South Wales',\n",
    "            'Bundanoon': 'New South Wales',\n",
    "            'Balmain': 'New South Wales',\n",
    "            'Bendigo': 'Victoria',\n",
    "            'Busselton': 'Western Australia',\n",
    "            'Barraba': 'New South Wales',\n",
    "            'Ballina': 'New South Wales', \n",
    "            'Blackheath': 'New South Wales',\n",
    "            'Binalong Bay': 'Tasmania',\n",
    "            'Balgal Beach': 'Queensland',\n",
    "            'Bundalong': 'Victoria',\n",
    "            'Boonah': 'Queensland',\n",
    "            'Orange': 'New South Wales', \n",
    "            'Braddon': 'Australian Capital Territory',\n",
    "            'Blacktown': 'New South Wales',\n",
    "            'Beechworth': 'Victoria',\n",
    "            'Bundoora': 'Victoria',\n",
    "            'Beerwah': 'Queensland',\n",
    "            'Bermagui': 'New South Wales',\n",
    "            'Bargara - Innes Park':'Queensland',\n",
    "            'Barham': 'New South Wales',\n",
    "            'Broken Hill': 'New South Wales',\n",
    "            'Broome': 'Western Australia',\n",
    "            'Bowral - Mittagong': 'New South Wales',\n",
    "            'Brunswick East': 'Victoria',\n",
    "            'Darlinghurst': 'New South Wales',\n",
    "            'Drouin': 'Victoria',\n",
    "            'Canberra': 'Australian Capital Territory',\n",
    "            'Colo Vale': 'New South Wales',\n",
    "            'Corowa': 'New South Wales',\n",
    "            'Colac': 'Victoria',\n",
    "            'Crows Nest': 'Queensland',\n",
    "            'Central Coast': 'New South Wales',\n",
    "            'Coffs Harbour': 'New South Wales',\n",
    "            'Cessnock': 'New South Wales', \n",
    "            'Cairns' :'Queensland',\n",
    "            'Clunes': 'Victoria',\n",
    "            'Crafers - Bridgewater': 'South Australia',\n",
    "            'Camperdown': 'New South Wales',\n",
    "            'Cumbalum': 'New South Wales',\n",
    "            'Chatswood': 'New South Wales',\n",
    "            'Carlton' : 'Victoria',\n",
    "            'Cape Schanck': 'Victoria',\n",
    "            'Caulfield North': 'Victoria',\n",
    "            'Cudlee Creek': 'South Australia', \n",
    "            'Capalaba': 'Queensland',\n",
    "            'Cannington': 'Western Australia',\n",
    "            'Coral Sea': 'Queensland',\n",
    "            'Cedar Vale': 'Queensland',\n",
    "            'Castlemaine': 'Victoria', \n",
    "            'Churchill': 'Victoria',\n",
    "            'Dubbo': 'New South Wales',\n",
    "            'Darwin' : 'Northern Territory',\n",
    "            'Denman': 'New South Wales',\n",
    "            'Derby': 'Western Australia',\n",
    "            'Docklands': 'Victoria', \n",
    "            'Devonport': 'Tasmania',\n",
    "            'Drysdale - Clifton Springs':'Victoria',\n",
    "            'Dwellingup': 'Western Australia',\n",
    "            'Deloraine': 'Tasmania',\n",
    "            'Dee Why': 'New South Wales',\n",
    "            'Dunsborough': 'Western Australia', \n",
    "            'Ellenbrook': 'Western Australia',\n",
    "            'Evandale': 'Tasmania',\n",
    "            'East Jindabyne': 'New South Wales',\n",
    "            'Esk': 'Queensland',\n",
    "            'Elizabeth Grove': 'South Australia',\n",
    "            'Empire Bay': 'New South Wales', \n",
    "            'Euroa': 'Victoria',\n",
    "            'Emerald': 'Queensland',\n",
    "            'Forestville': 'New South Wales',\n",
    "            'Freeling': 'South Australia',\n",
    "            'Fraser Island': 'Queensland',\n",
    "            'Forster - Tuncurry': 'New South Wales',\n",
    "            'Geraldton': 'Western Australia',\n",
    "            'Greenbushes': 'Western Australia',\n",
    "            'Gawler': 'South Australia',\n",
    "            'Gununa': 'Queensland',\n",
    "            'Glebe': 'New South Wales',\n",
    "            'Ghan': 'Northern Territory',\n",
    "            'Gold Coast': 'Queensland',\n",
    "            'Gladstone': 'Queensland',\n",
    "            'Griffith': 'New South Wales',\n",
    "            'Geelong': 'Victoria',\n",
    "            'Garden Island Creek': 'Tasmania',\n",
    "            'Good Night': 'Queensland',\n",
    "            'Glass House Mountains': 'Queensland',\n",
    "            'Glen Aplin': 'Queensland',\n",
    "            'Geeveston': 'Tasmania',\n",
    "            'Hobart': 'Tasmania',\n",
    "            'Healesville': 'Victoria',\n",
    "            'Hamilton': 'Victoria',\n",
    "            'Highfields':'Queensland',\n",
    "            'Hervey Bay': 'Queensland',\n",
    "            'Horsham': 'Victoria',\n",
    "            'Helensburgh': 'New South Wales',\n",
    "            'Inverloch': 'Queensland',\n",
    "            'Innisfail': 'Queensland',\n",
    "            'Jimboomba - West': 'Queensland',\n",
    "            'Jimboomba' : 'Queensland', \n",
    "            'Joondalup': 'Western Australia',\n",
    "            'Kooralbyn': 'Queensland',\n",
    "            'Kirwans Bridge': 'Victoria',\n",
    "            'Kalgoorlie - Boulder': 'Western Australia',\n",
    "            'Kenmore' : 'Queensland',\n",
    "            'Katherine': 'Northern Territory',\n",
    "            'Kiama': 'New South Wales',\n",
    "            'Kinka Beach': 'Queensland',\n",
    "            'Kadina': 'South Australia',\n",
    "            'Kilmore': 'Victoria',\n",
    "            'Kojonup': 'Western Australia',\n",
    "            'Kurri Kurri - Weston': 'New South Wales',\n",
    "            'Kingston': 'Australian Capital Terrotory',\n",
    "            'Kingaroy': 'Queensland',\n",
    "            'Lennox Head':'New South Wales',       \n",
    "            'Launceston': 'Tasmania',\n",
    "            'Lismore': 'New South Wales',\n",
    "            'Lithgow': 'New South Wales',\n",
    "            'Lorne': 'Victoria',\n",
    "            'Lake Conjola': 'New Sout Wales',\n",
    "            'Lara': 'Victoria',\n",
    "            'Melbourne': 'Victoria',\n",
    "            'Mackay': 'Queensland',\n",
    "            'Macedon': 'Victoria',\n",
    "            'Maryknoll': 'Victoria',\n",
    "            'McLaren Vale': 'South Australia',\n",
    "            'Moree': 'New South Wales', \n",
    "            'Mooloolaba':'Queensland',\n",
    "            'Maitland': 'New South Wales',\n",
    "            'Melton': 'Victoria',\n",
    "            'Morayfield': 'Queensland',\n",
    "            'Mosman': 'New South Wales',\n",
    "            'Margaret River': 'Western Australia',\n",
    "            'Mount Cotton': 'Queensland', \n",
    "            'Murrumbeena': 'Victoria',\n",
    "            'Murray Bridge': 'South Australia',\n",
    "            'Merewether': 'New South Wales',\n",
    "            'Murrurundi': 'New South Wales',\n",
    "            'Nhulunbuy': 'Northern Terrotory',\n",
    "            'Mount Hill': 'Western Australia',\n",
    "            'Marong': 'Victoria',\n",
    "            'Moranbah': 'Queensland',\n",
    "            'Mitchell Park': 'Victoria', \n",
    "            'Maddens Plains': 'New South Wales',\n",
    "            'Mount Burnett': 'Victoria',\n",
    "            'Morwell': 'Victoria', \n",
    "            'Merredin': 'Western Australia',\n",
    "            'Maudsland': 'Queensalnd',\n",
    "            'Morisset - Cooranbong': 'New South Wales',\n",
    "            'Macleay Island': 'Queensland',\n",
    "            'Mataranka': 'Northern Territory',\n",
    "            'Moss Vale': 'New South Wales',\n",
    "            'Malua Bay': 'New South Wales',\n",
    "            'Mia Mia': 'Victoria',\n",
    "            'Mudgee': 'New South Wales',\n",
    "            'Mildura': 'Victoria',\n",
    "            'Newcastle': 'New South Wales',\n",
    "            'Medowie': 'New South Wales',\n",
    "            'NSW' : 'New South Wales',\n",
    "            'Nyah West': 'Victoria',\n",
    "            'Nelly Bay': 'Queensland',\n",
    "            'Nerang': 'Queensland',\n",
    "            'North Strathfield': 'New South Wales',\n",
    "            'Narooma': 'New South Wales',\n",
    "            'New Norfolk': 'Tasmania',\n",
    "            'Norseman': 'Western Australia',\n",
    "            'North South Australia': 'North South Australia', \n",
    "            'Northam': 'Western Australia',\n",
    "            'Nambour': 'Queensland',\n",
    "            'Newport': 'Victoria',\n",
    "            'Newman': 'Western Australia',\n",
    "            'Narromine': 'New South Wales',\n",
    "            'Napperby': 'South Australia',\n",
    "            'Ocean Shores': 'New South Wales',\n",
    "            'Nelson Bay - Corlette': 'New South Wales',\n",
    "            'Phillip': 'Australian Capital Territory', \n",
    "            'Pakenham': 'Victoria',\n",
    "            'Perth': 'Western Australia',\n",
    "            'Port Macquarie': 'New South Wales',\n",
    "            'Port Vincent': 'South Australia',\n",
    "            'Port Broughton': 'South Australia',\n",
    "            'Port Douglas - Craiglie': 'Queensland',\n",
    "            'Port Victoria': 'Victoria',\n",
    "            'Port Lincoln': 'South Australia',\n",
    "            'Port Augusta': 'South Australia',\n",
    "            'Proserpine': 'Queensland',\n",
    "            'Port Pirie': 'South Australia',\n",
    "            'Porepunkah': 'Victoria',\n",
    "            'Pyrmont': 'New South Wales',\n",
    "            'Pacific Pines': 'Queensland', \n",
    "            'Palmwoods': 'Queensland',\n",
    "            'Queanbeyan' : 'New South Wales',\n",
    "            'Queenstown': 'Tasmania',\n",
    "            'Rockhampton': 'Queensland',\n",
    "            'Rosebud': 'Victoria',\n",
    "            'Rottnest Island': 'Western Australia',\n",
    "            'Redfern': 'New South Wales',\n",
    "            'Ravensthorpe': 'Western Australia',\n",
    "            'Rosebery': 'Tasmania',\n",
    "            'Southern River': 'Western Australia', \n",
    "            'Sydney' : 'New South Wales',\n",
    "            'Snake Valley': 'Victoria',\n",
    "            'Sunshine Coast': 'Queensland',\n",
    "            'Surf Beach - Sunderland Bay': 'Victoria',\n",
    "            'Stanthorpe': 'Queensland',\n",
    "            'Sandy Bay': 'Tasmania',\n",
    "            'Strathalbyn': 'South Australia',\n",
    "            'Silvan': 'Victoria',\n",
    "            'South Brisbane': 'Queensland',\n",
    "            'Scone': 'New South Wales', \n",
    "            'South Yarra': 'Victoria',\n",
    "            'Sale': 'Victoria',\n",
    "            'Summerlands': 'Victoria',\n",
    "            'Shepparton - Mooroopna': 'Victoria',\n",
    "            'Shoalhaven Heads': 'New South Wales',\n",
    "            'St Lucia': 'Queensland',\n",
    "            'St Marys': 'Tasmania',\n",
    "            'Stanwell Park': 'New South Wales',\n",
    "            'Sunbury': 'Victoria',\n",
    "            'Tumut': 'New South Wales',\n",
    "            'Two Wells': 'South Australia',\n",
    "            'Tamworth': 'New South Wales',\n",
    "            'Tintenbar': 'New South Wales',\n",
    "            'The Rocks': 'New South Wales',\n",
    "            'Torquay - Jan Juc': 'Victoria',\n",
    "            'Townsville': 'Queensland',\n",
    "            'Tamborine Mountain': 'Queensland',\n",
    "            'Tweed Heads': 'New South Wales',\n",
    "            'Thursday Island': 'Queensland',\n",
    "            'Temora': 'New South Wales',\n",
    "            'Toowoomba': 'Queensland',\n",
    "            'The University of South Australia' : 'South Australia',\n",
    "            'Trafalgar': 'Victoria',\n",
    "            'Thirlmere': 'New South Wales',\n",
    "            'Traralgon': 'Victoria',\n",
    "            'Taree': 'New South Wales',\n",
    "            'Tatura': 'Victoria',\n",
    "            'Tahmoor': 'New South Wales',\n",
    "            'Ulmarra': 'New South Wales',\n",
    "            'Uralla': 'New South Wales',\n",
    "            'Ulverstone': 'Tasmania',\n",
    "            'Ulladulla': 'New South Wales',\n",
    "            'Victor Harbor - Goolwa': 'South Australia',\n",
    "            'Warragul': 'Victoria',\n",
    "            'Warrnambool' : 'Victoria',\n",
    "            'Wollongong': 'New South Wales',\n",
    "            'Wagga Wagga': 'New South Wales',\n",
    "            'Wangaratta': 'Victoria',\n",
    "            'Whyalla': 'South Australia',\n",
    "            'Wodonga': 'Victoria',\n",
    "            'Wingello': 'New South Wales',\n",
    "            'Wilberforce': 'New South Wales',\n",
    "            'Wauchope': 'New South Wales',\n",
    "            'Woori Yallock - Launching Place': 'Victoria',\n",
    "            'Willyaroo': 'South Australia',\n",
    "            'Warwick': 'Queensland',\n",
    "            'Walloon': 'Queensland',\n",
    "            'Yulara':  'Northern Territory',\n",
    "            'Yass': 'New South Wales',\n",
    "            'Yamba': 'New South Wales',\n",
    "            'Yarrabah': 'Queensland',\n",
    "            'Yarrawonga': 'Victoria'\n",
    "           }\n",
    "\n",
    "#df = pd.read_csv(path + '/data/Twitter/sample/Clean_covid/Aug20_basic.csv', \n",
    "#                    low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c8804-b4da-4d87-92f0-4edf17e72e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change names and save df --------- no longer required\n",
    "def change_name(filename):\n",
    "    df = pd.read_csv(path + '/data/Twitter/sample/clean_influenza/' +  filename + '.csv', \n",
    "                    low_memory=False, skipinitialspace=True) #,  engine='python' \n",
    "    df['State'] = df['GeoName'].replace(location, regex=True)\n",
    "    df['State'] = df['State'].str.replace(r\"\\(.*\\)\",\"\", regex=True)\n",
    "    #save data into csv\n",
    "    df.to_csv(path + '/data/Twitter/sample/clean_influenza/' + filename + '.csv', index = False) \n",
    "    \n",
    "    return  df['State'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43da72e-528c-406c-a78d-1f590919a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "newunique_states = change_name('Mar20_basic')\n",
    "newunique_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e34ee4-2474-4c5c-9519-14c6649065e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0293c002-d469-4877-92f1-c8bb95713cfc",
   "metadata": {},
   "source": [
    "## Group Tweets by weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c756db8f-c8ad-4b00-bff8-c8cd80ac7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_byweek(filename):\n",
    "    df = pd.read_csv(path + '/data/Twitter/sample/clean_influenza/' +  filename + '.csv',\n",
    "                     low_memory=False, skipinitialspace=True)\n",
    "    #format Date column as datetime \n",
    "    #df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d') #use this for influenza  \n",
    "    # df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y') #use this for influenza  \n",
    "    #extract required columns \n",
    "    df_sub = df[['Date', 'State']]\n",
    "    df_sub = df_sub.copy()  #this avoids warning\n",
    "    df_sub.loc[:,'Count'] = 1   #add a column with 1s\n",
    "    #insert weeks\n",
    "    df_sub.loc[:,'Week'] = df_sub['Date'].dt.to_period('W').dt.to_timestamp()\n",
    "    #groupby weeks\n",
    "    df_grp = df_sub.groupby(['State', 'Week'], as_index=False).sum(numeric_only=True)\n",
    "    return df_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7f8a7-2163-458d-92d0-c893de798ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path + '/data/Twitter/sample/clean_influenza/Jan18_basic.csv',\n",
    "#                      low_memory=False, skipinitialspace=True)\n",
    "# df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y') #use this for influenza\n",
    "# df_sub = df[['Date', 'State']]\n",
    "# df_sub = df_sub.copy()  #this avoids warning\n",
    "# df_sub.loc[:,'Count'] = 1   #add a column with 1s\n",
    "# # # #insert weeks\n",
    "# df_sub.loc[:,'Week'] = df_sub['Date'].dt.to_period('W-THU').dt.start_time\n",
    "# df_grp = df_sub.groupby(['State', 'Week'], as_index=False).sum(numeric_only=True)\n",
    "\n",
    "# #df_sub.to_csv(path + '/data/Twitter/sample/clean_influenza/df_sub_Week2.csv', index = False)\n",
    "\n",
    "# df_sub.dtypes\n",
    "\n",
    "# #df_grp['State'].str.strip()  #--- this doesn't work\n",
    "# print(df_grp['State'].unique())\n",
    "# df_grp.loc[df_grp['State'] == 'Queensland']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905913d4-7014-4b8d-8a3b-ba5f19783757",
   "metadata": {},
   "source": [
    "## Prepare dataframes for each State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a334bf3a-eb5f-4c46-90e4-f13b1df80625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Week</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [State, Week, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ACT = pd.DataFrame(columns=['State', 'Week', 'Count'])\n",
    "df_NSW = pd.DataFrame(columns=['State', 'Week', 'Count'])\n",
    "df_NT = pd.DataFrame(columns=['State', 'Week', 'Count'])\n",
    "df_QLD= pd.DataFrame(columns=['State', 'Week', 'Count'])\n",
    "df_SA = pd.DataFrame(columns=['State', 'Week', 'Count'])\n",
    "df_TAS = pd.DataFrame(columns=['State', 'Week', 'Count'])\n",
    "df_VIC= pd.DataFrame(columns=['State', 'Week', 'Count'])\n",
    "df_WA = pd.DataFrame(columns=['State', 'Week', 'Count'])\n",
    "df_WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd6838bf-423f-45ca-847a-8828e45358e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_df(filename):\n",
    "    \n",
    "    global df_ACT, df_NSW, df_NT, df_QLD, df_SA, df_TAS, df_VIC, df_WA\n",
    "    #get the grouped by week  data\n",
    "    df_grp = group_byweek(filename)\n",
    "     \n",
    "    \n",
    "    #filter states\n",
    "    df_ACT_sub = df_grp.loc[df_grp['State'] == 'Australian Capital Territory']\n",
    "    df_NSW_sub = df_grp.loc[df_grp['State'] == 'New South Wales']\n",
    "    df_NT_sub = df_grp.loc[df_grp['State'] == 'Northern Territory']\n",
    "    df_QLD_sub = df_grp.loc[df_grp['State'] == 'Queensland']\n",
    "    df_SA_sub = df_grp.loc[df_grp['State'] == 'South Australia']\n",
    "    df_TAS_sub = df_grp.loc[df_grp['State'] == 'Tasmania']\n",
    "    df_VIC_sub = df_grp.loc[df_grp['State'] == 'Victoria']\n",
    "    df_WA_sub = df_grp.loc[df_grp['State'] == 'Western Australia']\n",
    "    \n",
    "    #add rows to end of dfs already created \n",
    "    if not df_ACT_sub.empty:\n",
    "           df_ACT = pd.concat([df_ACT, df_ACT_sub], ignore_index = True)\n",
    "    if not df_NSW_sub.empty:\n",
    "           df_NSW = pd.concat([df_NSW, df_NSW_sub], ignore_index = True)\n",
    "    if not df_NT_sub.empty:\n",
    "           df_NT = pd.concat([df_NT, df_NT_sub], ignore_index = True)\n",
    "    if not df_QLD_sub.empty:\n",
    "           df_QLD = pd.concat([df_QLD, df_QLD_sub], ignore_index = True)\n",
    "    if not df_SA_sub.empty:\n",
    "           df_SA = pd.concat([df_SA, df_SA_sub], ignore_index = True)\n",
    "    if not df_TAS_sub.empty:\n",
    "           df_TAS = pd.concat([df_TAS, df_TAS_sub], ignore_index = True)\n",
    "    if not df_VIC_sub.empty:\n",
    "           df_VIC = pd.concat([df_VIC, df_VIC_sub], ignore_index = True)\n",
    "    if not df_WA_sub.empty:\n",
    "           df_WA = pd.concat([df_WA, df_WA_sub], ignore_index = True)\n",
    "    \n",
    "    #save data into csv\n",
    "    df_ACT.to_csv(path + '/data/Twitter/sample/clean_influenza/df_ACT.csv', index = False)\n",
    "    df_NSW.to_csv(path + '/data/Twitter/sample/clean_influenza/df_NSW.csv', index = False)\n",
    "    df_NT.to_csv(path + '/data/Twitter/sample/clean_influenza/df_NT.csv', index = False)\n",
    "    df_QLD.to_csv(path + '/data/Twitter/sample/clean_influenza/df_QLD.csv', index = False)\n",
    "    df_SA.to_csv(path + '/data/Twitter/sample/clean_influenza/df_SA.csv', index = False)\n",
    "    df_TAS.to_csv(path + '/data/Twitter/sample/clean_influenza/df_TAS.csv', index = False)\n",
    "    df_VIC.to_csv(path + '/data/Twitter/sample/clean_influenza/df_VIC.csv', index = False)\n",
    "    df_WA.to_csv(path + '/data/Twitter/sample/clean_influenza/df_WA.csv', index = False)\n",
    "    \n",
    "    return  df_ACT.shape, df_NSW.shape, df_NT.shape,  df_QLD.shape, df_SA.shape, df_TAS.shape, df_VIC.shape, df_WA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dad907eb-7506-4f39-bc84-b31ca898a21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3), (14, 3), (1, 3), (13, 3), (11, 3), (4, 3), (14, 3), (11, 3))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df= get_state_df('Mar18_basic')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55825d-cf0f-4fb4-8850-1f63662e86a9",
   "metadata": {},
   "source": [
    "Note: the above grouping creates dfs with repeated dates. \n",
    "So merge those rows and add the counts for those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3e0d89e5-064d-4fc5-bf42-f498052cbc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State            object\n",
      "Week     datetime64[ns]\n",
      "Count             int64\n",
      "dtype: object\n",
      "(258, 3)\n",
      "False\n",
      "True\n",
      "(237, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Week</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>2022-12-26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 State       Week  Count\n",
       "0    Western Australia 2018-01-01      1\n",
       "1    Western Australia 2018-01-08      3\n",
       "2    Western Australia 2018-01-15      1\n",
       "3    Western Australia 2018-01-29      3\n",
       "4    Western Australia 2018-02-05      2\n",
       "..                 ...        ...    ...\n",
       "232  Western Australia 2022-11-28      2\n",
       "233  Western Australia 2022-12-05      9\n",
       "234  Western Australia 2022-12-12      2\n",
       "235  Western Australia 2022-12-19      5\n",
       "236  Western Australia 2022-12-26      5\n",
       "\n",
       "[237 rows x 3 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'df_WA'\n",
    "df = pd.read_csv(path + '/data/Twitter/sample/clean_influenza/' +  filename + '.csv',\n",
    "                     low_memory=False, parse_dates = ['Week'], dayfirst =True)\n",
    "#     #format Date column as datetime \n",
    "# df['Week'] = pd.to_datetime(df['Week'], format='%d/%m/%y')\n",
    "# df['Week'] = pd.to_datetime(df['Week'], format='%Y-%m-%d') #use this for influenza  \n",
    "print(df.dtypes)\n",
    "print(df.shape)\n",
    "print(df['Week'].is_unique)\n",
    "df\n",
    "\n",
    "# df2 = df.copy()  #this avoids warning\n",
    "df2 = df.groupby(['State','Week',],as_index=False).sum(numeric_only=True) #numeric_only=True\n",
    "print(df2['Week'].is_unique)  #check if Week column has  unique values\n",
    "print(df2.shape)\n",
    "df2.to_csv(path + '/data/Twitter/sample/clean_influenza/' +filename + '.csv', index = False)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b3b13-67df-4083-bd24-18662cea50f0",
   "metadata": {},
   "source": [
    "# NNDSS Influenza data \n",
    "\n",
    "- Original Data is recorded at the end of the week on Fridays. \n",
    "- Data for ACT is not included \n",
    "- Data is available upto end of 2021\n",
    "- data from: https://www.health.gov.au/resources/publications/national-notifiable-diseases-surveillance-system-nndss-public-dataset-influenza-laboratory-confirmed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11180fb-0be7-4932-afa4-7f87e02846a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>newWeek</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW</td>\n",
       "      <td>2016-02-15</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW</td>\n",
       "      <td>2016-02-22</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW</td>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>WA</td>\n",
       "      <td>2021-11-08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>WA</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>WA</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>WA</td>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>WA</td>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1922 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     State    newWeek  Count\n",
       "0      NSW 2016-02-15    141\n",
       "1      NSW 2016-02-22    149\n",
       "2      NSW 2016-02-29    173\n",
       "3      NSW 2016-03-07    173\n",
       "4      NSW 2016-03-14    152\n",
       "...    ...        ...    ...\n",
       "1917    WA 2021-11-08      2\n",
       "1918    WA 2021-11-22      2\n",
       "1919    WA 2021-12-06      2\n",
       "1920    WA 2021-12-13      1\n",
       "1921    WA 2021-12-20      1\n",
       "\n",
       "[1922 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data and \n",
    "df = pd.read_csv(path + '/data/NNDSS_influenza/influenza_data.csv',\n",
    "                     low_memory=False, skipinitialspace=True)\n",
    "#print(df['Week'][0])#day/month/year \n",
    "#change week to date format \n",
    "df['Week'] = pd.to_datetime(df['Week'], format='%d/%m/%Y') #use this for influenza\n",
    "\n",
    "df.loc[:,'Count'] = 1   #add a column with 1s\n",
    "#  #format week so that it starts from monday of week\n",
    "df.loc[:,'newWeek'] = df['Week'].dt.to_period('W').dt.to_timestamp()\n",
    "# df.to_csv(path + '/data/NNDSS_influenza/influenza_data_newWeek.csv', index = False)\n",
    "df_grp = df.groupby(['State', 'newWeek'], as_index=False).sum(numeric_only=True)\n",
    "# #df_grp['State'].unique()  #['NSW', 'Qld', 'SA', 'Vic', 'WA', 'NT', 'Tas']\n",
    "# #df_grp\n",
    "# #df.dtypes\n",
    "df_grp\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccb162ef-9cbf-4395-bb22-16b26bf9b844",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>newWeek</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>WA</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>WA</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>WA</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>WA</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>WA</td>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>WA</td>\n",
       "      <td>2019-11-25</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>WA</td>\n",
       "      <td>2019-12-02</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>WA</td>\n",
       "      <td>2019-12-09</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>WA</td>\n",
       "      <td>2019-12-16</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>WA</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     State    newWeek  Count\n",
       "1747    WA 2018-01-01     53\n",
       "1748    WA 2018-01-08     52\n",
       "1749    WA 2018-01-15     67\n",
       "1750    WA 2018-01-22     66\n",
       "1751    WA 2018-01-29     97\n",
       "...    ...        ...    ...\n",
       "1846    WA 2019-11-25     51\n",
       "1847    WA 2019-12-02     55\n",
       "1848    WA 2019-12-09     69\n",
       "1849    WA 2019-12-16     42\n",
       "1850    WA 2019-12-23     57\n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###check missing instrances\n",
    "check = df_grp[(df_grp['newWeek']<= '2019-12-23') & (df_grp['newWeek']>='2018-01-01')]  ## should have 104 rows \n",
    "check ## 730 rows\n",
    "check.loc[check['State'] == 'NSW'] #104\n",
    "check.loc[check['State'] == 'NT'] #99, missing 5\n",
    "check.loc[check['State'] == 'Qld'] #104\n",
    "check.loc[check['State'] == 'SA']##104\n",
    "check.loc[check['State'] == 'Tas']##104\n",
    "check.loc[check['State'] == 'Vic']##104\n",
    "check.loc[check['State'] == 'WA']##104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb3c03-41a7-4c76-923a-c93c7de917b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_infldf():\n",
    "    \n",
    "    global df_grp, df_NSW, df_NT, df_QLD, df_SA, df_TAS, df_VIC, df_WA\n",
    "        \n",
    "    \n",
    "    #filter states\n",
    "    df_NSW_sub = df_grp.loc[df_grp['State'] == 'NSW']\n",
    "    df_NT_sub = df_grp.loc[df_grp['State'] == 'NT']\n",
    "    df_QLD_sub = df_grp.loc[df_grp['State'] == 'Qld']\n",
    "    df_SA_sub = df_grp.loc[df_grp['State'] == 'SA']\n",
    "    df_TAS_sub = df_grp.loc[df_grp['State'] == 'Tas']\n",
    "    df_VIC_sub = df_grp.loc[df_grp['State'] == 'Vic']\n",
    "    df_WA_sub = df_grp.loc[df_grp['State'] == 'WA']\n",
    "    \n",
    "    #add rows to end of dfs already created \n",
    "    if not df_NSW_sub.empty:\n",
    "           df_NSW = pd.concat([df_NSW, df_NSW_sub], ignore_index = True)\n",
    "    if not df_NT_sub.empty:\n",
    "           df_NT = pd.concat([df_NT, df_NT_sub], ignore_index = True)\n",
    "    if not df_QLD_sub.empty:\n",
    "           df_QLD = pd.concat([df_QLD, df_QLD_sub], ignore_index = True)\n",
    "    if not df_SA_sub.empty:\n",
    "           df_SA = pd.concat([df_SA, df_SA_sub], ignore_index = True)\n",
    "    if not df_TAS_sub.empty:\n",
    "           df_TAS = pd.concat([df_TAS, df_TAS_sub], ignore_index = True)\n",
    "    if not df_VIC_sub.empty:\n",
    "           df_VIC = pd.concat([df_VIC, df_VIC_sub], ignore_index = True)\n",
    "    if not df_WA_sub.empty:\n",
    "           df_WA = pd.concat([df_WA, df_WA_sub], ignore_index = True)\n",
    "    \n",
    "    #save data into csv\n",
    "    df_NSW.to_csv(path + '/data/NNDSS_influenza/df_NSW.csv', index = False)\n",
    "    df_NT.to_csv(path + '/data/NNDSS_influenza/df_NT.csv', index = False)\n",
    "    df_QLD.to_csv(path + '/data/NNDSS_influenza/df_QLD.csv', index = False)\n",
    "    df_SA.to_csv(path + '/data/NNDSS_influenza/df_SA.csv', index = False)\n",
    "    df_TAS.to_csv(path + '/data/NNDSS_influenza/df_TAS.csv', index = False)\n",
    "    df_VIC.to_csv(path + '/data/NNDSS_influenza/df_VIC.csv', index = False)\n",
    "    df_WA.to_csv(path + '/data/NNDSS_influenza/df_WA.csv', index = False)\n",
    "    \n",
    "    return   df_NSW.shape, df_NT.shape,  df_QLD.shape, df_SA.shape, df_TAS.shape, df_VIC.shape, df_WA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4b063-d4b8-4437-8881-74333b352ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= get_state_infldf()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda56891-448e-4a6c-aade-370f53177669",
   "metadata": {},
   "source": [
    "## Get weekly nnds data from all states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb5bc95-6cb7-4594-aa79-ff429923fd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newWeek</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2018</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/10/2018</td>\n",
       "      <td>2297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/11/2021</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/2/2021</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/3/2021</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     newWeek  Count\n",
       "0   1/1/2018    662\n",
       "1  1/10/2018   2297\n",
       "2  1/11/2021      9\n",
       "3   1/2/2021     13\n",
       "4   1/3/2021     13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path + '/data/NNDSS_influenza/influenza_data_newWeek.csv',\n",
    "                     low_memory=False, skipinitialspace=True)\n",
    "df.head()\n",
    "#print(df['Week'][0])#day/month/year \n",
    "df_grp = df.groupby(['newWeek'], as_index=False).sum(numeric_only=True)\n",
    "df_grp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb9013-6c7a-4ef8-88d7-8841c84e1f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ace52dee-7cff-4f19-a5c6-822b92a5d296",
   "metadata": {},
   "source": [
    "# CRISPER Covid data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d7f98-81c3-498d-ae48-f9bdbac1cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + '/data/Crisper/cases/covid19data_au_state_cases_new.csv',\n",
    "                     low_memory=False, skipinitialspace=True)\n",
    "# print(df['date'][0])#day/month/year \n",
    "# print(type(df['date'][0]))\n",
    "\n",
    "#change week to date format \n",
    "df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y') \n",
    "df.loc[:,'week'] = df['date'].dt.to_period('W').dt.to_timestamp()\n",
    "df['state'].unique() #['WA', 'VIC', 'TAS', 'SA', 'QLD', 'NT', 'NSW', 'ACT']\n",
    "df.to_csv(path + '/data/Crisper/covid19cases_week.csv', index = False)\n",
    "df_grp = df.groupby(['state', 'week'], as_index=False).sum(numeric_only=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc935295-160f-4d1e-8e47-d70c81fed2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACT = pd.DataFrame(columns=['state', 'week', 'cases'])\n",
    "df_NSW = pd.DataFrame(columns=['state', 'week', 'cases'])\n",
    "df_NT = pd.DataFrame(columns=['state',  'week', 'cases'])\n",
    "df_QLD= pd.DataFrame(columns=['state',  'week', 'cases'])\n",
    "df_SA = pd.DataFrame(columns=['state',  'week', 'cases'])\n",
    "df_TAS = pd.DataFrame(columns=['state',  'week', 'cases'])\n",
    "df_VIC= pd.DataFrame(columns=['state',  'week', 'cases'])\n",
    "df_WA = pd.DataFrame(columns=['state',  'week', 'cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1adce-af54-4ecb-b2a5-ff7cd00bfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_covid_df():\n",
    "    \n",
    "    global df_grp, df_ACT, df_NSW, df_NT, df_QLD, df_SA, df_TAS, df_VIC, df_WA\n",
    "        \n",
    "  \n",
    "    #filter states\n",
    "    df_ACT_sub = df_grp.loc[df_grp['state'] == 'ACT']\n",
    "    df_NSW_sub = df_grp.loc[df_grp['state'] == 'NSW']\n",
    "    df_NT_sub = df_grp.loc[df_grp['state'] == 'NT']\n",
    "    df_QLD_sub = df_grp.loc[df_grp['state'] == 'QLD']\n",
    "    df_SA_sub = df_grp.loc[df_grp['state'] == 'SA']\n",
    "    df_TAS_sub = df_grp.loc[df_grp['state'] == 'TAS']\n",
    "    df_VIC_sub = df_grp.loc[df_grp['state'] == 'VIC']\n",
    "    df_WA_sub = df_grp.loc[df_grp['state'] == 'WA']\n",
    "    \n",
    "    #add rows to end of dfs already created \n",
    "    if not df_ACT_sub.empty:\n",
    "           df_ACT = pd.concat([df_ACT, df_ACT_sub], ignore_index = True)\n",
    "    if not df_NSW_sub.empty:\n",
    "           df_NSW = pd.concat([df_NSW, df_NSW_sub], ignore_index = True)\n",
    "    if not df_NT_sub.empty:\n",
    "           df_NT = pd.concat([df_NT, df_NT_sub], ignore_index = True)\n",
    "    if not df_QLD_sub.empty:\n",
    "           df_QLD = pd.concat([df_QLD, df_QLD_sub], ignore_index = True)\n",
    "    if not df_SA_sub.empty:\n",
    "           df_SA = pd.concat([df_SA, df_SA_sub], ignore_index = True)\n",
    "    if not df_TAS_sub.empty:\n",
    "           df_TAS = pd.concat([df_TAS, df_TAS_sub], ignore_index = True)\n",
    "    if not df_VIC_sub.empty:\n",
    "           df_VIC = pd.concat([df_VIC, df_VIC_sub], ignore_index = True)\n",
    "    if not df_WA_sub.empty:\n",
    "           df_WA = pd.concat([df_WA, df_WA_sub], ignore_index = True)\n",
    "    \n",
    "    #save data into csv\n",
    "    df_ACT.to_csv(path + '/data/Crisper/cases/df_ACT.csv', index = False)\n",
    "    df_NSW.to_csv(path + '/data/Crisper/cases/df_NSW.csv', index = False)\n",
    "    df_NT.to_csv(path + '/data/Crisper/cases/df_NT.csv', index = False)\n",
    "    df_QLD.to_csv(path + '/data/Crisper/cases/df_QLD.csv', index = False)\n",
    "    df_SA.to_csv(path + '/data/Crisper/cases/df_SA.csv', index = False)\n",
    "    df_TAS.to_csv(path + '/data/Crisper/cases/df_TAS.csv', index = False)\n",
    "    df_VIC.to_csv(path + '/data/Crisper/cases/df_VIC.csv', index = False)\n",
    "    df_WA.to_csv(path + '/data/Crisper/cases/df_WA.csv', index = False)\n",
    "    \n",
    "    return   df_ACT.shape, df_NSW.shape, df_NT.shape,  df_QLD.shape, df_SA.shape, df_TAS.shape, df_VIC.shape, df_WA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eaa051-85c7-456e-9aba-6ccc59198f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= get_state_covid_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f527b2-a6cf-423d-bde5-c3ef28c997de",
   "metadata": {},
   "source": [
    "## Google Trends Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d353312-c691-414f-9460-c4cc702d0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>covid</th>\n",
       "      <th>influenza</th>\n",
       "      <th>Haemophilus influenzae</th>\n",
       "      <th>flu</th>\n",
       "      <th>parainfluenza</th>\n",
       "      <th>H1N1</th>\n",
       "      <th>H7N9</th>\n",
       "      <th>H5N1</th>\n",
       "      <th>...</th>\n",
       "      <th>metapneumovirus</th>\n",
       "      <th>Bordetella pertussis</th>\n",
       "      <th>Mycoplasma pneumoniae</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>bronchitis</th>\n",
       "      <th>H9N2</th>\n",
       "      <th>sinusitis</th>\n",
       "      <th>upper respiratory tract infection</th>\n",
       "      <th>Tamiflu</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>64</td>\n",
       "      <td>14</td>\n",
       "      <td>2018-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>69</td>\n",
       "      <td>18</td>\n",
       "      <td>2018-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2022-12-04</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>23</td>\n",
       "      <td>60</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>2022-11-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2022-12-11</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2022-12-18</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2022-12-25</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2022-12-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  coronavirus  covid  influenza  Haemophilus influenzae  flu  \\\n",
       "0   2018-01-07            0      0          7                      26    6   \n",
       "1   2018-01-14            0      0          6                       0    5   \n",
       "2   2018-01-21            0      0          7                      70    5   \n",
       "3   2018-01-28            0      0          9                       0    6   \n",
       "4   2018-02-04            0      0          8                       0    5   \n",
       "..         ...          ...    ...        ...                     ...  ...   \n",
       "256 2022-12-04            0      8          7                      20    6   \n",
       "257 2022-12-11            0      8          8                       0    7   \n",
       "258 2022-12-18            0      9         10                       0    7   \n",
       "259 2022-12-25            0      7          8                       0    6   \n",
       "260 2023-01-01            0      6          7                       0    5   \n",
       "\n",
       "     parainfluenza  H1N1  H7N9  H5N1  ...  metapneumovirus  \\\n",
       "0                0     0    76    10  ...                0   \n",
       "1               17     0    46     0  ...                8   \n",
       "2                0     0     0     0  ...                0   \n",
       "3                0     0     0     0  ...                0   \n",
       "4                0     0     0    10  ...                0   \n",
       "..             ...   ...   ...   ...  ...              ...   \n",
       "256             25     0     0     6  ...               40   \n",
       "257             29     2     0     0  ...               15   \n",
       "258             32     5     0    19  ...               44   \n",
       "259              0     3     0     0  ...               34   \n",
       "260             13     0     0     0  ...               13   \n",
       "\n",
       "     Bordetella pertussis  Mycoplasma pneumoniae  pneumonia  bronchitis  H9N2  \\\n",
       "0                       0                      0         18          39     0   \n",
       "1                       0                     31         15          26     0   \n",
       "2                       0                      0         15          17     0   \n",
       "3                       0                      0         19          29     0   \n",
       "4                       0                      0         16          31     0   \n",
       "..                    ...                    ...        ...         ...   ...   \n",
       "256                    40                      0         23          45    23   \n",
       "257                     0                      0         22          56     0   \n",
       "258                     0                     23         23          50     0   \n",
       "259                     0                      0         20          42     0   \n",
       "260                     0                      0         19          36     0   \n",
       "\n",
       "     sinusitis  upper respiratory tract infection  Tamiflu       Week  \n",
       "0           61                                  0        7 2018-01-01  \n",
       "1           44                                 64       14 2018-01-08  \n",
       "2           32                                 24       13 2018-01-15  \n",
       "3           34                                 80        9 2018-01-22  \n",
       "4           34                                 69       18 2018-01-29  \n",
       "..         ...                                ...      ...        ...  \n",
       "256         60                                 19        8 2022-11-28  \n",
       "257         69                                  0        4 2022-12-05  \n",
       "258         56                                 26        0 2022-12-12  \n",
       "259         56                                  0       12 2022-12-19  \n",
       "260         52                                  0        9 2022-12-26  \n",
       "\n",
       "[261 rows x 26 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path + '/data/GoogleTrends/AU.csv',\n",
    "                     low_memory=False)  #, parse_dates=['date']\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d') #format='%d/%m/%y'\n",
    "#  #format week so that it starts from monday of week\n",
    "df.loc[:,'Week'] = df['date'].dt.to_period('W').dt.to_timestamp()\n",
    "df.to_csv(path + '/data/GoogleTrends/df_AU.csv', index = False)\n",
    "\n",
    "print(df['date'].dtypes)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20a7eb-0b99-488d-b750-45e12e8bad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['date'][0])\n",
    "#df.shape\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b656ad6-793c-44ad-8366-801003e5c0eb",
   "metadata": {},
   "source": [
    "#### Date: 04 Sept 2023 \n",
    "## Prepare NNDSS and Google Trends data for spatiotemporal modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ce6edbe-1542-4fc2-b661-006c15bbbccd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 23)\n",
      "['ACT']\n",
      "(261, 23)\n",
      "['NSW']\n",
      "(261, 21)\n",
      "['NT']\n",
      "(261, 23)\n",
      "['QLD']\n",
      "(261, 23)\n",
      "['SA']\n",
      "(261, 23)\n",
      "['TAS']\n",
      "(261, 23)\n",
      "['VIC']\n",
      "(261, 23)\n",
      "['WA']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>State</th>\n",
       "      <th>influenza</th>\n",
       "      <th>Haemophilus influenzae</th>\n",
       "      <th>parainfluenza</th>\n",
       "      <th>H1N1</th>\n",
       "      <th>H7N9</th>\n",
       "      <th>H5N1</th>\n",
       "      <th>H3N2</th>\n",
       "      <th>grippe</th>\n",
       "      <th>...</th>\n",
       "      <th>rhinovirus</th>\n",
       "      <th>respiratory syncytial virus</th>\n",
       "      <th>metapneumovirus</th>\n",
       "      <th>Mycoplasma pneumoniae</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>Bordetella pertussis</th>\n",
       "      <th>bronchitis</th>\n",
       "      <th>H9N2</th>\n",
       "      <th>upper respiratory tract infection</th>\n",
       "      <th>Tamiflu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>NT</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>NT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>NT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>NT</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>NT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date State  influenza  Haemophilus influenzae  parainfluenza  H1N1  \\\n",
       "0  2018-01-07    NT         29                       0              0     0   \n",
       "1  2018-01-14    NT          0                       0              0    43   \n",
       "2  2018-01-21    NT          0                       0              0     0   \n",
       "3  2018-01-28    NT         37                       0              0     0   \n",
       "4  2018-02-04    NT          0                       0              0     0   \n",
       "\n",
       "   H7N9  H5N1  H3N2  grippe  ...  rhinovirus  respiratory syncytial virus  \\\n",
       "0     0     0     0       0  ...           0                            0   \n",
       "1     0     0     0       0  ...           0                            0   \n",
       "2     0     0     0       0  ...           0                            0   \n",
       "3     0     0     0       0  ...           0                            0   \n",
       "4     0     0     0       0  ...          29                            0   \n",
       "\n",
       "   metapneumovirus  Mycoplasma pneumoniae  pneumonia  Bordetella pertussis  \\\n",
       "0                0                      0          0                     0   \n",
       "1                0                      0          0                     0   \n",
       "2                0                      0          0                     0   \n",
       "3                0                      0          0                     0   \n",
       "4                0                      0         18                     0   \n",
       "\n",
       "   bronchitis  H9N2  upper respiratory tract infection  Tamiflu  \n",
       "0           0     0                                  0        0  \n",
       "1          28     0                                  0        0  \n",
       "2          36     0                                  0        0  \n",
       "3           0     0                                  0        0  \n",
       "4           0     0                                  0        0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a function to read GT data and insert state\n",
    "def read_data(filename, statename):\n",
    "    df = pd.read_csv(path + '/data/GoogleTrends/' + filename + '.csv')\n",
    "#drop unwanted columns. The unwanted columns are for covid search terms \n",
    "    df = df.drop(['coronavirus', 'covid'], axis=1)\n",
    "    print(df.shape) #(261, 23)\n",
    "    df.insert(1,'State',statename)  #add state \n",
    "     # df_agg.insert(0,'SA3Code',df_agg['SA3Region'].replace(region_to_sa3_mapping))\n",
    "    print(df['State'].unique())\n",
    "    # print(df.dtypes) #date is in object format\n",
    "    df.head()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# filename = 'AU-ACT'\n",
    "df_act = read_data('AU-ACT', 'ACT')\n",
    "df_nsw = read_data('AU-NSW', 'NSW')\n",
    "df_nt = read_data('AU-NT', 'NT') #no data for 'flu' and 'sinusitis'\n",
    "df_qld = read_data('AU-QLD', 'QLD')\n",
    "df_sa = read_data('AU-SA', 'SA')\n",
    "df_tas = read_data('AU-TAS', 'TAS')\n",
    "df_vic = read_data('AU-VIC', 'VIC')\n",
    "df_wa = read_data('AU-WA', 'WA')\n",
    "\n",
    "\n",
    "df_nt.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45471f21-a724-4ad1-90be-52170b530224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2088, 24)\n",
      "['ACT' 'NSW' 'NT' 'QLD' 'SA' 'TAS' 'VIC' 'WA']\n"
     ]
    }
   ],
   "source": [
    "#vertically concatenate dfs \n",
    "df_gt =  pd.concat([df_act, df_nsw, df_nt, df_qld, df_sa, df_tas, df_vic, df_wa], axis=0)\n",
    "df_gt['date'] = pd.to_datetime(df_gt['date'], dayfirst = True)\n",
    "df_gt.insert(1,'Year', df_gt['date'].dt.strftime('%Y')) #add year column \n",
    "df_gt['Year'] = df_gt['Year'].astype('int')\n",
    "print(df_gt.shape) #(2088, 24) , 261*8 = 2088\n",
    "print(df_gt['State'].unique())\n",
    "#save gt data \n",
    "# df_gt.to_csv(path + '/data/GoogleTrends/df_gt_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11048b0e-3e43-4066-872a-19cb6089b0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                                 datetime64[ns]\n",
      "Year                                          int64\n",
      "State                                        object\n",
      "influenza                                     int64\n",
      "Haemophilus influenzae                        int64\n",
      "flu                                         float64\n",
      "parainfluenza                                 int64\n",
      "H1N1                                          int64\n",
      "H7N9                                          int64\n",
      "H5N1                                          int64\n",
      "H3N2                                          int64\n",
      "grippe                                        int64\n",
      "gripe                                         int64\n",
      "adenovirus                                    int64\n",
      "rhinovirus                                    int64\n",
      "respiratory syncytial virus                   int64\n",
      "metapneumovirus                               int64\n",
      "Bordetella pertussis                          int64\n",
      "Mycoplasma pneumoniae                         int64\n",
      "pneumonia                                     int64\n",
      "bronchitis                                    int64\n",
      "H9N2                                          int64\n",
      "sinusitis                                   float64\n",
      "upper respiratory tract infection             int64\n",
      "Tamiflu                                       int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>influenza</th>\n",
       "      <th>Haemophilus influenzae</th>\n",
       "      <th>flu</th>\n",
       "      <th>parainfluenza</th>\n",
       "      <th>H1N1</th>\n",
       "      <th>H7N9</th>\n",
       "      <th>H5N1</th>\n",
       "      <th>...</th>\n",
       "      <th>respiratory syncytial virus</th>\n",
       "      <th>metapneumovirus</th>\n",
       "      <th>Bordetella pertussis</th>\n",
       "      <th>Mycoplasma pneumoniae</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>bronchitis</th>\n",
       "      <th>H9N2</th>\n",
       "      <th>sinusitis</th>\n",
       "      <th>upper respiratory tract infection</th>\n",
       "      <th>Tamiflu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Year State  influenza  Haemophilus influenzae   flu  \\\n",
       "0 2018-01-07  2018   ACT          0                       0   4.0   \n",
       "1 2018-01-14  2018   ACT         19                       0   6.0   \n",
       "2 2018-01-21  2018   ACT          0                       0   8.0   \n",
       "3 2018-01-28  2018   ACT          0                       0  11.0   \n",
       "4 2018-02-04  2018   ACT          0                       0   3.0   \n",
       "\n",
       "   parainfluenza  H1N1  H7N9  H5N1  ...  respiratory syncytial virus  \\\n",
       "0              0   100    28     0  ...                            0   \n",
       "1             20     0     0    24  ...                            0   \n",
       "2              0     0    32     0  ...                            0   \n",
       "3              0    36     0     0  ...                            0   \n",
       "4              0     0    41     0  ...                            0   \n",
       "\n",
       "   metapneumovirus  Bordetella pertussis  Mycoplasma pneumoniae  pneumonia  \\\n",
       "0                0                     0                      0          0   \n",
       "1                0                     0                      0          0   \n",
       "2                0                     0                      0          0   \n",
       "3                0                     0                      0          0   \n",
       "4               21                     0                      0          0   \n",
       "\n",
       "   bronchitis  H9N2  sinusitis  upper respiratory tract infection  Tamiflu  \n",
       "0           0     0       54.0                                  0        0  \n",
       "1           0     0       19.0                                  0       92  \n",
       "2           0     0        0.0                                  0       90  \n",
       "3           0     0        0.0                                  0        0  \n",
       "4           0     0        0.0                                  0        0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gt.to_csv(path + '/data/GoogleTrends/df_gt_all.csv', index=False)\n",
    "\n",
    "print(df_gt.dtypes)\n",
    "df_gt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768cee6-c695-465b-98e0-62f7690470f9",
   "metadata": {},
   "source": [
    "#### Read Formatted Flu data for all states from NNDSS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bb25071-f1b7-4cf0-993d-7affe8ac1cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(724919, 4)\n",
      "(389682, 4)\n",
      "Week       datetime64[ns]\n",
      "State              object\n",
      "Count               int64\n",
      "newWeek            object\n",
      "dtype: object\n",
      "['NSW' 'NT' 'QLD' 'SA' 'TAS' 'VIC' 'WA']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>State</th>\n",
       "      <th>Count</th>\n",
       "      <th>newWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335237</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NSW</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335238</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NSW</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335239</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NSW</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335240</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NSW</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335241</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NSW</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Week State  Count   newWeek\n",
       "335237 2018-01-05   NSW      1  1/1/2018\n",
       "335238 2018-01-05   NSW      1  1/1/2018\n",
       "335239 2018-01-05   NSW      1  1/1/2018\n",
       "335240 2018-01-05   NSW      1  1/1/2018\n",
       "335241 2018-01-05   NSW      1  1/1/2018"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: ACT is not in this dataset, and data is available up to end of 2021\n",
    "df_nndss = pd.read_csv(path + '/data/NNDSS_influenza/influenza_data_newWeek.csv')\n",
    "print(df_nndss.shape)#(724919, 4)\n",
    "df_nndss['Week'] = pd.to_datetime(df_nndss['Week'], dayfirst=True) #convert to datetime\n",
    "df_nndss = df_nndss[~(df_nndss['Week'] < '2018-01-01')]\n",
    "print(df_nndss.shape)#(389682, 4)\n",
    "print(df_nndss.dtypes)\n",
    "print(df_nndss['State'].unique())\n",
    "df_nndss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11aee224-c647-4128-92e2-c3d516c856c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1241, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>State</th>\n",
       "      <th>totalCount</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NSW</td>\n",
       "      <td>182</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NT</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>QLD</td>\n",
       "      <td>199</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>SA</td>\n",
       "      <td>85</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>TAS</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Week State  totalCount  Year\n",
       "0 2018-01-05   NSW         182  2018\n",
       "1 2018-01-05    NT           7  2018\n",
       "2 2018-01-05   QLD         199  2018\n",
       "3 2018-01-05    SA          85  2018\n",
       "4 2018-01-05   TAS           2  2018"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregate count based on date\n",
    "df_agg = df_nndss.groupby(['Week','State']).agg(\n",
    "         totalCount = ('Count','sum')).reset_index()\n",
    "print(df_agg.shape) #1241, 3, shape of df_gt = (1827, 24), #nndss data is available 2021-12-31\n",
    "\n",
    "df_agg['Year'] = df_agg['Week'].dt.strftime('%Y') #add year column\n",
    "df_agg.head()\n",
    "\n",
    "\n",
    "#save flu \n",
    "df_agg.to_csv(path + '/data/NNDSS_influenza/df_nndss_all.csv', index=False)\n",
    "# #check data\n",
    "# df_agg = pd.read_csv(path + '/data/NNDSS_influenza/df_nndss_all.csv')\n",
    "# df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6005d700-891f-4951-911e-1dedf1a6d52f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State    object\n",
      "Year     object\n",
      "Popn      int64\n",
      "dtype: object\n",
      "(32, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>Popn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW</td>\n",
       "      <td>2018</td>\n",
       "      <td>8003564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIC</td>\n",
       "      <td>2018</td>\n",
       "      <td>6479695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QLD</td>\n",
       "      <td>2018</td>\n",
       "      <td>5046434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SA</td>\n",
       "      <td>2018</td>\n",
       "      <td>1755715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WA</td>\n",
       "      <td>2018</td>\n",
       "      <td>2636404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  Year     Popn\n",
       "0   NSW  2018  8003564\n",
       "1   VIC  2018  6479695\n",
       "2   QLD  2018  5046434\n",
       "3    SA  2018  1755715\n",
       "4    WA  2018  2636404"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read population data \n",
    "df_pop = pd.read_csv(path + '/data/population/state_territory_popn.csv')\n",
    "df_pop= df_pop.drop(['2022'], axis=1)\n",
    "df_pop = pd.melt(df_pop, id_vars=[\"State\"], value_vars=[\"2018\", \"2019\",\"2020\", \"2021\"],\n",
    "           var_name=\"Year\", value_name=\"Popn\") #convert to long form\n",
    " \n",
    "print(df_pop.dtypes)\n",
    "print(df_pop.shape) #68 by 3\n",
    "#8 states, 4 yrs = 32\n",
    "# df_pop.to_csv(path + '/data/population/df_pop.csv', index=False)\n",
    "\n",
    "df_pop.head()\n",
    "\n",
    "#merge df_agg and df_pop\n",
    "# df_agg.merge(df_pop, how='left', on=['State', 'Year'])\n",
    "df_merged = df_agg.merge(df_pop, left_on=['State', 'Year'], right_on = ['State', 'Year'],\n",
    "                         how='left')\n",
    "\n",
    "print(df_merged.shape)\n",
    "# df_merged.to_csv(path + '/data/NNDSS_influenza/df_merged.csv', index=False)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343d201-7fb3-41b1-a720-0e027031ace0",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d30e1f98-f50b-4e6e-832b-0f3169b181c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['NSW', 'NT', 'QLD', 'SA', 'TAS', 'VIC', 'WA'], dtype=object), array(['2018', '2019', '2020', '2021'], dtype=object)]\n",
      "[array(['NSW', 'VIC', 'QLD', 'SA', 'WA', 'TAS', 'NT', 'ACT'], dtype=object), array(['2018', '2019', '2020', '2021'], dtype=object)]\n",
      "[array(['NSW', 'NT', 'QLD', 'SA', 'TAS', 'VIC', 'WA'], dtype=object), array(['2018', '2019', '2020', '2021'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_agg.shape #(1241, 4)\n",
    "print([df_agg['State'].unique(), df_agg['Year'].unique()])\n",
    "print([df_pop['State'].unique(), df_pop['Year'].unique()])\n",
    "print([df_merged['State'].unique(), df_merged['Year'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14740afe-812b-44b6-81e0-4e6cf0e359e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarise flu data\n",
    "print(df_agg['totalCount'].agg(['min', 'max','mean','std']))\n",
    "# min        1.000000\n",
    "# max     8652.000000\n",
    "# mean     314.006446\n",
    "# std      855.257851\n",
    "\n",
    "#for nndss, data is from 2018 to 2021\n",
    "print(min(df_nndss['Week'].unique())) #2018-01-05 Friday\n",
    "print(max(df_nndss['Week'].unique())) #2021-12-31, Friday\n",
    "#for ggooglr trends, data is from 2018 to 202\n",
    "print(min(df_gt['date'].unique())) #2018-01-07, saturday\n",
    "print(max(df_gt['date'].unique())) #2023-01-01, Wednesday "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf2b5b-5807-45f1-ab5b-9a710597b4a4",
   "metadata": {},
   "source": [
    "#### Combine googleTrend and flu data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47546a95-f03e-4887-b237-9480aef225b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Add a date column to df_merged (which is flu data) by adding two days to each element of \n",
    "Week column. This would align flu dates to gt dates as Gt data are recorded on Saturday of the week and flu data are recorded on Friday of the week.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18258d42-6699-4a0a-ace5-51d00bedcd7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>State</th>\n",
       "      <th>totalCount</th>\n",
       "      <th>Year</th>\n",
       "      <th>Popn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NSW</td>\n",
       "      <td>182</td>\n",
       "      <td>2018</td>\n",
       "      <td>8003564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NT</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "      <td>245920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>QLD</td>\n",
       "      <td>199</td>\n",
       "      <td>2018</td>\n",
       "      <td>5046434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>SA</td>\n",
       "      <td>85</td>\n",
       "      <td>2018</td>\n",
       "      <td>1755715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>TAS</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>542927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Week State  totalCount  Year     Popn\n",
       "0 2018-01-05   NSW         182  2018  8003564\n",
       "1 2018-01-05    NT           7  2018   245920\n",
       "2 2018-01-05   QLD         199  2018  5046434\n",
       "3 2018-01-05    SA          85  2018  1755715\n",
       "4 2018-01-05   TAS           2  2018   542927"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c853dc26-60ed-487f-a9bb-ab66aa303e22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>date</th>\n",
       "      <th>State</th>\n",
       "      <th>totalCount</th>\n",
       "      <th>Year</th>\n",
       "      <th>Popn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>NSW</td>\n",
       "      <td>182</td>\n",
       "      <td>2018</td>\n",
       "      <td>8003564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>NT</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "      <td>245920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>QLD</td>\n",
       "      <td>199</td>\n",
       "      <td>2018</td>\n",
       "      <td>5046434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>SA</td>\n",
       "      <td>85</td>\n",
       "      <td>2018</td>\n",
       "      <td>1755715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>TAS</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>542927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Week       date State  totalCount  Year     Popn\n",
       "0 2018-01-05 2018-01-07   NSW         182  2018  8003564\n",
       "1 2018-01-05 2018-01-07    NT           7  2018   245920\n",
       "2 2018-01-05 2018-01-07   QLD         199  2018  5046434\n",
       "3 2018-01-05 2018-01-07    SA          85  2018  1755715\n",
       "4 2018-01-05 2018-01-07   TAS           2  2018   542927"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_merged['date'] = df_merged['Week'] + pd.DateOffset(days=2)\n",
    "df_merged.insert(1,'date', df_merged['Week']+ pd.DateOffset(days=2))\n",
    "\n",
    "df_merged.head()\n",
    "\n",
    "# #check difference between two dates \n",
    "# diff = (df_merged['date'] - df_merged['Week']).dt.days\n",
    "# diff\n",
    "# diff.unique() #array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "332eb55a-7d99-4458-a24a-898360f1dccd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>influenza</th>\n",
       "      <th>Haemophilus influenzae</th>\n",
       "      <th>flu</th>\n",
       "      <th>parainfluenza</th>\n",
       "      <th>H1N1</th>\n",
       "      <th>H7N9</th>\n",
       "      <th>H5N1</th>\n",
       "      <th>...</th>\n",
       "      <th>respiratory syncytial virus</th>\n",
       "      <th>metapneumovirus</th>\n",
       "      <th>Bordetella pertussis</th>\n",
       "      <th>Mycoplasma pneumoniae</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>bronchitis</th>\n",
       "      <th>H9N2</th>\n",
       "      <th>sinusitis</th>\n",
       "      <th>upper respiratory tract infection</th>\n",
       "      <th>Tamiflu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Year State  influenza  Haemophilus influenzae   flu  \\\n",
       "0 2018-01-07  2018   ACT          0                       0   4.0   \n",
       "1 2018-01-14  2018   ACT         19                       0   6.0   \n",
       "2 2018-01-21  2018   ACT          0                       0   8.0   \n",
       "3 2018-01-28  2018   ACT          0                       0  11.0   \n",
       "4 2018-02-04  2018   ACT          0                       0   3.0   \n",
       "\n",
       "   parainfluenza  H1N1  H7N9  H5N1  ...  respiratory syncytial virus  \\\n",
       "0              0   100    28     0  ...                            0   \n",
       "1             20     0     0    24  ...                            0   \n",
       "2              0     0    32     0  ...                            0   \n",
       "3              0    36     0     0  ...                            0   \n",
       "4              0     0    41     0  ...                            0   \n",
       "\n",
       "   metapneumovirus  Bordetella pertussis  Mycoplasma pneumoniae  pneumonia  \\\n",
       "0                0                     0                      0          0   \n",
       "1                0                     0                      0          0   \n",
       "2                0                     0                      0          0   \n",
       "3                0                     0                      0          0   \n",
       "4               21                     0                      0          0   \n",
       "\n",
       "   bronchitis  H9N2  sinusitis  upper respiratory tract infection  Tamiflu  \n",
       "0           0     0       54.0                                  0        0  \n",
       "1           0     0       19.0                                  0       92  \n",
       "2           0     0        0.0                                  0       90  \n",
       "3           0     0        0.0                                  0        0  \n",
       "4           0     0        0.0                                  0        0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e56cc929-90ff-4691-bb80-3039480d013d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2088, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Year_x</th>\n",
       "      <th>State</th>\n",
       "      <th>influenza</th>\n",
       "      <th>Haemophilus influenzae</th>\n",
       "      <th>flu</th>\n",
       "      <th>parainfluenza</th>\n",
       "      <th>H1N1</th>\n",
       "      <th>H7N9</th>\n",
       "      <th>H5N1</th>\n",
       "      <th>...</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>bronchitis</th>\n",
       "      <th>H9N2</th>\n",
       "      <th>sinusitis</th>\n",
       "      <th>upper respiratory tract infection</th>\n",
       "      <th>Tamiflu</th>\n",
       "      <th>Week</th>\n",
       "      <th>totalCount</th>\n",
       "      <th>Year_y</th>\n",
       "      <th>Popn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Year_x State  influenza  Haemophilus influenzae   flu  \\\n",
       "0 2018-01-07    2018   ACT          0                       0   4.0   \n",
       "1 2018-01-14    2018   ACT         19                       0   6.0   \n",
       "2 2018-01-21    2018   ACT          0                       0   8.0   \n",
       "3 2018-01-28    2018   ACT          0                       0  11.0   \n",
       "4 2018-02-04    2018   ACT          0                       0   3.0   \n",
       "\n",
       "   parainfluenza  H1N1  H7N9  H5N1  ...  pneumonia  bronchitis  H9N2  \\\n",
       "0              0   100    28     0  ...          0           0     0   \n",
       "1             20     0     0    24  ...          0           0     0   \n",
       "2              0     0    32     0  ...          0           0     0   \n",
       "3              0    36     0     0  ...          0           0     0   \n",
       "4              0     0    41     0  ...          0           0     0   \n",
       "\n",
       "   sinusitis  upper respiratory tract infection  Tamiflu  Week  totalCount  \\\n",
       "0       54.0                                  0        0   NaT         NaN   \n",
       "1       19.0                                  0       92   NaT         NaN   \n",
       "2        0.0                                  0       90   NaT         NaN   \n",
       "3        0.0                                  0        0   NaT         NaN   \n",
       "4        0.0                                  0        0   NaT         NaN   \n",
       "\n",
       "   Year_y  Popn  \n",
       "0     NaN   NaN  \n",
       "1     NaN   NaN  \n",
       "2     NaN   NaN  \n",
       "3     NaN   NaN  \n",
       "4     NaN   NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine df_merge and df_gt\n",
    "# df_agg.merge(df_pop, how='left', on=['State', 'Year'])\n",
    "df_gt_flu = df_gt.merge(df_merged, left_on=['date', 'State'], right_on = ['date', 'State'],\n",
    "                         how='left')\n",
    "\n",
    "print(df_gt_flu.shape)\n",
    "\n",
    "df_gt_flu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5b2b24d-2978-44d4-a2ff-dd5f28293ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_gt_flu.to_csv(path + '/data/combined/gt_flu.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (DigDisSurv)",
   "language": "python",
   "name": "venv_digdissurv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
